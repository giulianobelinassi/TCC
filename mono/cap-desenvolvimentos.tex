%% ------------------------------------------------------------------------- %%
\chapter{Development}

The first goal of the project was to compile and execute the code with at least one sample. 
From this point we found a few issues such as:

\begin{itemize}
\item The original code provided by Carrion had issues common to legacy code. 
It depended on some features offered by Compaq Visual Fortran, thus the code needed 
modifications to be able to compile with the GFortran compiler. The code also had 
indentation issues and used several GOTO statements to execute simple tasks such 
as looping because of limitations imposed by Fortran 77.

\item There were no automated tests for the program.

\item Fortran has a feature called implicit variable declaration that showed to be 
error-prone because the code had a bug due to a misspelled variable in a file 
named \texttt{Solfundif.for} (the \texttt{RDN} variable was misspelled \texttt{DRN}),
and the compiler did not complain about the uninitialized variable.

\item There was a need to call CUDA code from Fortran to avoid a complete rewrite of the 
application in C or C++. The Portland Group (PGI) provides a paid CUDA Fortran compiler,
so another solution was required.

\item The original code only used static memory allocation for loading all data. 
This resulted in the following problems: (1) Huge memory usage for small inputs because the 
application does not know the memory requirement to load the data, and (2) A limit of 2Gb of 
memory for the total of statically allocated variables \cite{intel_mcmodel:2010}. 

\end{itemize}


After all those issues were addressed, A parallel optimization of BEM began by analyzing the sequential code provided. 
Gprof, a profiling tool by \cite{binutils}, revealed the two most time-consuming routines: 
$\texttt{Ghmatecd}$ and $\texttt{Nonsingd}$, with $60.9\%$ and $58.3\%$ of the program total 
elapsed time, respectively.
Since most calls to $\texttt{Nonsingd}$ were performed inside $\texttt{Ghmatecd}$, most
of the parallelization effort was focused 
on that last routine.


\section{Calling CUDA from Fortran 77}

Although The Portland Group (PGI) provides a CUDA Fortran compiler, it is not freeware software 
and thus might increase the application development costs. An alternative to this is to 
create C++ functions that are callable from Fortran and use CUDA C++ to program the kernels.

Since GFortran appends an underscore to all compiled Fortran subroutines, manually appending an 
underscore to a C routine makes it visible to Fortran. Since Fortran also passes all arguments 
as reference, then the C routine needs to expect a pointer to a variable rather than the variable 
itself. Now, to call C++ from Fortran it is necessary to just wrap the function with \texttt{extern "C"}
clause, as exemplified below.

\begin{minipage}{\textwidth}
\lstset{language=C}
\begin{lstlisting}
extern "C"{
	void callable_from_fortran_(int* a, int* b){
		printf("%d\n", (*a) + (*b));
	}
}
\end{lstlisting}
\lstset{language=Fortran}
\begin{lstlisting}
      PROGRAM CALL_C
        INTERFACE
          SUBROUTINE callable_from_fortran(a, b)
            IMPLICIT NONE
            INTEGER a, b
          END SUBROUTINE
        END INTERFACE

        CALL callable_from_fortran(40, 2)
      END PROGRAM CALL_C
\end{lstlisting}
\end{minipage}
It must be highlighted that matrices in Fortran are stored in column-major order and indexed from 1, 
wherein C and C++ are stored in line-major order and indexed from 0. This means that when accessing 
the position \texttt{A(i,j)} from a Fortran matrix in C, one must access \texttt{A[j-1][i-1]}.

\section{$\texttt{Ghmatecd}$ Optimization and Parallelization}

$\texttt{Ghmatecd}$ is a routine developed to create both the $H$ and $G$ matrices for the dynamic problem 
described in equation ($\ref{eqmatrix}$). It does so by making several calls to \texttt{Nonsingd}, a routine
that uses the Gaussian Quadrature as described by chapter $\ref{cap:quadrature}$ to compute the integral of the 
dynamic part in equation ($\ref{bem_formulation}$); and \texttt{Sing\_de}, a routine 
that implements the artifice described by equation ($\ref{singular}$).

At first, we did some optimization to the original sequential code to remove unnecessary computations or memory copy.
\texttt{Ghmatecd}'s code had additional matrices used in the computations that dependency analysis showed to be unnecessary. 
There was also a vectorial product that was computed for each column of $H$ and $G$ that were recomputed in other 
routines, thus it was moved to a separate function called \texttt{Normvec} and its result passed to \texttt{Ghmatecd}.

We also optimized \texttt{Nonsingd} routine by removing all calls to a procedure named \texttt{Gauleg} that 
were used to compute the abscissae and weight points of the Gaussian Quadrature. These points are always the 
zeroes of the Legendre Polynomials and there is no need to compute them more than once in the entire program, 
hence the result of \texttt{Gauleg} can just be passed to \texttt{Nonsingd}.

Algorithm 1 shows pseudocode for the $\texttt{Ghmatecd}$ subroutine. Let $n$ be the number of mesh 
elements and $m$ the number of boundary elements. $\texttt{Ghmatecd}$ builds matrices $H$ and $G$ by computing 
smaller $3\times3$ matrices returned by $\texttt{Nonsingd}$ and $\texttt{Sing\_de}$.
%
\begin{algorithm}[H]
\label{ghmatecd_old}
\caption{Creates $H, G \in \Cfield^{(3m)\times(3n)}$}
\begin{algorithmic}[1]
	\Procedure{Ghmatecd}{}
		\For{$j := 1, n$} 
			\For{$i := 1, m$}
				\State{$ii := 3(i-1) + 1;     jj := 3(j-1) + 1$}
				\If{$i == j$}
					\State{$Gelement, Helement \leftarrow \text{Sing\_de}(i)$}\Comment{two $3\times3$ complex matrices}					
				\Else
					\State{$Gelement, Helement \leftarrow \text{Nonsingd}(i, j)$}	
				\EndIf
				\State{$G[ii:ii+2][jj:jj+2] \leftarrow Gelement$}
				\State{$H[ii:ii+2][jj:jj+2] \leftarrow Helement$}
			\EndFor
	 \EndFor
	\EndProcedure
\end{algorithmic}
\end{algorithm}
%
There is no interdependency between all iterations of the loops in lines 2 and 3, so all iterations can be computed 
in parallel. With OpenMP, this can be done with a simple \texttt{pragma for} statement. Since typically a modern high-end 
CPU have 8 cores, even a small number of mesh elements generate enough 
workload to use all CPUs resources if this strategy alone is used. On the other hand, a typical GPU contain thousands 
of processors, hence even a considerable large amount of elements may not generate a workload that consumes all the device's 
resources. Since $\texttt{Nonsingd}$ is the cause of the performance bottleneck of $\texttt{Ghmatecd}$, 
the main effort was implementing
an optimized version of $\texttt{Ghmatecd}$, called $\texttt{Ghmatecd\_Nonsingd}$, that only computes the $\texttt{Nonsingd}$
case in the GPU, and leave $\texttt{Sing\_de}$ to be computed in the CPU after the computation of $\texttt{Ghmatecd\_Nonsingd}$ 
is completed. The pseudocode in Algorithm \ref{ghmatecd_new} pictures a new strategy where $\texttt{Nonsingd}$ is also computed in parallel.
Let $g$ be the number of Gauss quadrature points.

\begin{algorithm}[H]
\caption{Creates $H, G \in \Cfield^{(3m)\times(3n)}$ }
\label{ghmatecd_new}
\begin{algorithmic}[1]
	\Procedure{Ghmatecd\_nonsingd}{}
		\For{$j := 1, n$} 
			\For{$i := 1, m$}
				\State{$ii := 3(i-1) + 1;     jj := 3(j-1) + 1$}
				\State{Allocate \textit{Hbuffer} and \textit{Gbuffer}, buffer of matrices $3 \times 3$ of size $g^2$}
				\If{$i \neq j$}
					\For{$y := 1, g$}
						\For{$x := 1, g$}
							\State{$\textit{Hbuffer}(x, y) \leftarrow \text{GenerateMatrixH}(i, j, x, y)$}
							\State{$\textit{Gbuffer}(x, y) \leftarrow \text{GenerateMatrixG}(i, j, x, y)$}
						\EndFor
					\EndFor
				\EndIf
				\State{$Gelement \leftarrow \text{SumAllMatricesInBuffer}(\textit{Gbuffer})$} 
				\State{$Helement \leftarrow \text{SumAllMatricesInBuffer}(\textit{Hbuffer})$}
				\State{$G[ii:ii+2][jj:jj+2] \leftarrow Gelement$}
				\State{$H[ii:ii+2][jj:jj+2] \leftarrow Helement$}
			\EndFor
	 \EndFor
	\EndProcedure
	\Procedure{Ghmatecd\_Sing\_de}{}
		\For{$i := 1, m$}
			\State{$ii := 3(i-1) + 1$}
			\State{$Gelement, Helement \leftarrow \text{Sing\_de}(i)$}	
			\State{$G[ii:ii+2][ii:ii+2] \leftarrow Gelement$}
			\State{$H[ii:ii+2][ii:ii+2] \leftarrow Helement$}
	 \EndFor
	\EndProcedure
	\Procedure{Ghmatecd}{} 
		\State{$\text{Ghmatecd\_Nonsingd}()$}
		\State{$\text{Ghmatecd\_Sing\_de}()$}
	\EndProcedure
		
\end{algorithmic}
\end{algorithm}
%\begin{algorithm}
%\caption{Creates $H, G \in \Cfield^{(3m)\times(3n)}$ without Sing_de part}
%\begin{algorithmic}[1]
%	\Procedure{Ghmatecd\_nonsingd}{}
%		\For{$j := 1, n$} 
%			\For{$i := 1, m$}
%				\State{$ii := 3(i-1) + 1;     jj := 3(j-1) + 1$}
%				\State{Allocate \textit{Hbuffer} and \textit{Gbuffer}, buffer of matrices $3 \times 3$ of size $g^2$}
%				\If{$i \neq j$}
%					\For{$y := 1, g$}
%						\For{$x := 1, g$}
%							\State{$\textit{Hbuffer}(x, y) \leftarrow \text{GenerateMatrixH}(i, j, x, y)$}
%							\State{$\textit{Gbuffer}(x, y) \leftarrow \text{GenerateMatrixG}(i, j, x, y)$}
%						\EndFor
%					\EndFor
%					\State{$Gelement \leftarrow \text{SumAllMatricesInBuffer}(\textit{Gbuffer})$} 
%					\State{$Helement \leftarrow \text{SumAllMatricesInBuffer}(\textit{Hbuffer})$}
%					\State{$G[ii:ii+2][jj:jj+2] \leftarrow Gelement$}
%					\State{$H[ii:ii+2][jj:jj+2] \leftarrow Helement$}
%				\EndIf
%			\EndFor
%	 \EndFor
%	\EndProcedure
%\end{algorithmic}
%\end{algorithm}
%
%\begin{algorithm}
%\caption{Compute Sing_de part of $H, G \in \Cfield^{(3m)\times(3n)}$}
%\begin{algorithmic}[1]
%	\Procedure{Ghmatecd\_Sing_de}{}
%		\For{$i := 1, m$}
%			\State{$ii := 3(i-1) + 1$}
%			\State{$Gelement, Helement \leftarrow \text{Sing_de}(i)$}	
%			\State{$G[ii:ii+2][ii:ii+2] \leftarrow Gelement$}
%			\State{$H[ii:ii+2][ii:ii+2] \leftarrow Helement$}
%	 \EndFor
%	\EndProcedure
%\end{algorithmic}
%\end{algorithm}

The $\texttt{Ghmatecd\_Nonsingd}$ routine can be implemented as a CUDA kernel. In a CUDA block, $g \times g$ 
threads are created to compute in parallel the two nested loops in lines 2 and 3, allocating spaces in the shared 
memory to keep the matrix buffers $\texttt{Hbuffer}$ and $\texttt{Gbuffer}$. Since these buffers contain matrices of size 
$3 \times 3$, nine of these $g \times g$ threads can be used to 
sum all matrices, because one thread can be assigned to each matrix entry, unless $g < 3$. Note that $g$ is also upper-bounded by the 
amount of shared memory available in the GPU. Launching $m \times n$ blocks to cover the two nested loops in lines
2 to 3 will generate the entire $H$ and $G$ without the $\texttt{Sing\_de}$ part. The $\texttt{Ghmatecd\_Sing\_de}$ routine can be parallelized with 
a simple OpenMP $\texttt{Parallel for}$ clause, and it will compute the remaining $H$ and $G$. 

A careful examination of \texttt{Sing\_de} concluded that, algorithmically, it is very similar to \texttt{Nonsingd} 
with a special logic to overcome singularity issues. If we merge both procedures in one, then there would 
be no need to compute the diagonal of $H$ and $G$ using the CPU. Algorithm $\ref{ghmatecd_new2}$ captures this idea.

\begin{algorithm}[H]
\caption{Creates $H, G \in \Cfield^{(3m)\times(3n)}$ }
\label{ghmatecd_new2}
\begin{algorithmic}[1]
	\Procedure{Ghmatecd}{}
		\For{$j := 1, n$} 
			\For{$i := 1, m$}
				\State{$ii := 3(i-1) + 1;     jj := 3(j-1) + 1$}
				\State{Allocate \textit{Hbuffer} and \textit{Gbuffer}, buffer of matrices $3 \times 3$ of size $g^2$}
				\For{$y := 1, g$}
					\For{$x := 1, g$}
						\State{$\textit{Hbuffer}(x, y) \leftarrow \text{GenerateMatrixH}(i, j, x, y)$}
						\State{$\textit{Gbuffer}(x, y) \leftarrow \text{GenerateMatrixG}(i, j, x, y)$}
						\If{$i == j$}
							\State{\text{OvercomeSingularity}($i, \&\textit{Hbuffer}(x, y), \&\textit{Gbuffer}(x, y))$}
						\EndIf
					\EndFor
				\EndFor
				\State{$Gelement \leftarrow \text{SumAllMatricesInBuffer}(\textit{Gbuffer})$} 
				\State{$Helement \leftarrow \text{SumAllMatricesInBuffer}(\textit{Hbuffer})$}
				\State{$G[ii:ii+2][jj:jj+2] \leftarrow Gelement$}
				\State{$H[ii:ii+2][jj:jj+2] \leftarrow Helement$}
			\EndFor
	 \EndFor
	\EndProcedure
\end{algorithmic}
\end{algorithm}

Algorithm $\ref{ghmatecd_new2}$ can be implemented as a CUDA kernel analogously as \texttt{Ghmatecd\_Nonsingd}. This strategy has the advantage 
of only using the GPU to compute the entire $H$ and $G$ and there are no idle blocks as in Algorithm $\ref{ghmatecd_new}$, but tests show a 
significant loss of precision in this version when compared to the previous one.

\section{$\texttt{Ghmatece}$ Optimization and Parallelization}

$\texttt{Ghmatece}$ is a routine developed to create both the $H$ and $G$ matrices for the static problem described in equation ($\ref{eqmatrix}$). 
Unless by \texttt{RIGID} routine, a procedure designed to calculate $H$ diagonal by considerating the rigid body movement, 
it is very similar to \texttt{Ghmatecd} routine as described by Algorithm \ref{ghmatece_old}. 

\begin{algorithm}[H]
\caption{Creates $H, G \in \Rfield^{(3m)\times(3n)}$}
\label{ghmatece_old}
\begin{algorithmic}[1]
	\Procedure{Ghmatece}{}
		\For{$j := 1, n$} 
			\For{$i := 1, m$}
				\State{$ii := 3(i-1) + 1;     jj := 3(j-1) + 1$}
				\If{$i == j$}
					\State{$Gelement, Helement \leftarrow \text{Singge}(i)$}\Comment{two $3\times3$ real matrices}					
				\Else
					\State{$Gelement, Helement \leftarrow \text{Nonsinge}(i, j)$}	
				\EndIf
				\State{$G[ii:ii+2][jj:jj+2] \leftarrow Gelement$}
				\State{$H[ii:ii+2][jj:jj+2] \leftarrow Helement$}
			\EndFor
		\EndFor 
		\State{$\text{Rigid}(H)$}
	\EndProcedure

	\Procedure{Rigid}{H}
		\For{$i := 1, m$} 
			\For{$j := 1, n$}
				\State{$ii := 3(i-1) + 1;     jj := 3(j-1) + 1$}
				\If{$i \neq j$}
					\State{$H[ii:ii+2][ii:ii+2] \leftarrow H[ii:ii+2][ii:ii+2] + H[ii:ii+2][jj:jj+2]$}
				\EndIf
			\EndFor
		\EndFor
		\EndProcedure
\end{algorithmic}
\end{algorithm}

The parallelization technique presented in Algorithm $\ref{ghmatecd_new}$ can be applied to \texttt{Ghmatece} 
because \texttt{Nonsinge} operates in the same way as \texttt{Nonsingd}, but it computes the integral 
of the static nuclei in the equation ($\ref{bem_formulation}$). Unfortunately, since 
\texttt{Singge} makes 4 calls to \texttt{Nonsinge}, the strategy presented in 
algorithm $\ref{ghmatecd_new2}$ cannot be deployed efficiently because it would 
generate too much workload to a group of blocks.

For the \texttt{RIGID} procedure, it has no interdependency between lines and thus it can be parallelized using the 
\texttt{!\$OMP} \texttt{PARALLEL} \texttt{DO} pragma.

\section{Linear System Solving Optimization}
In order to compute all regions surface forces and displacements, a linear system can be assembled 
from the matrices $H$ and $G$ from the dynamic problem, that means solving: 
\begin{equation}
	Ax = b
\end{equation}
Where $A$ is a square nonsingular complex matrix. As described in chapter $\ref{cap:lu}$, 
the LU decomposition with partial pivoting can be deployed to solve it.

The original code used a sequential implementation provided by Compaq within a routine called \texttt{CGESV}. In order to explore multiple CPUs,
a library called OpenBLAS implements this routine taking advantage of multicore architectures.  

A Library named MAGMA implements the LU decomposition with GPU acceleration within a function named \texttt{magma\_cgesv\_gpu} that shows good results for this application.

