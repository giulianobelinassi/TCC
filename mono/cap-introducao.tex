%% ------------------------------------------------------------------------- %%
\chapter{Introdução}
\label{cap:introducao}


Differential equations governing problems of Mathematical Physics have analytical 
solutions only in cases in which the domain geometry, boundary and initial conditions are
reasonably simple. Problems with arbitrary domains and fairly general boundary conditions 
can only be solved approximately, for example, by using numerical techniques. 
These techniques were strongly developed due to the presence of increasingly powerful computers, 
enabling the solution of complex mathematical problems.

The Boundary Element Method (BEM) is a very efficient alternative for modeling unlimited domains since
it satisfies the Sommerfeld radiation condition, also known as geometric damping 
\cite{katsikadelis:2016}. This method can be used for numerically modeling the stationary behavior of 3D 
wave propagation in the soil and it is useful as a computational tool to aid in the analysis of soil vibration
\cite{dominguez:1993}. A BEM based tool can be used for analyzing the vibration created 
by heavy machines, railway lines, earthquakes, or even to aid the design of offshore oil platforms.

With the advent of GPUs, several mathematical and engineering simulation problems were redesigned
to be implemented into these massively parallel devices. However, 
first GPUs were designed to render graphics in real time, as a consequence, all the available 
libraries, such as OpenGL, were graphical oriented. These redesigns involved converting 
the original problem to the graphics domain and required expert knowledge of the selected 
graphical library. 

NVIDIA noticed a new demand for their products and created an API called CUDA to enable 
the use of GPUs for general purpose programming. CUDA uses the concept of kernels, which are
functions called from the host to be executed by GPU threads. Kernels are organized into a set of blocks composed of a set 
of threads that cooperate with each other \cite{patterson:2007}.

The memory of a NVIDIA GPU is divided in global memory, local memory, and shared memory.
Global memory is accessible by all threads, local memory is private to a thread and shared 
memory is low-latency and accessible by all threads in a block\cite{patterson:2007}. CUDA 
provides mechanisms to access all of them.

Regarding this work, this parallelization approach is useful because an analysis of a large domain 
requires a proportionally large number of mesh elements, and processing a single element have a high
time cost. Doing such analysis in parallel reduces the computational time requied for the entire
program because multiple elements are processed at the same time. This advantage was provided by this
research.

\textbf{Talk about the structure of this paper}.

Before discussing any parallelization technique or results, Section 2 presents a very brief mathematical 
description of BEM for Stationary Elastodynamic Problems and the meaning of some functions presented in 
this paper. Section 3 shows how the most computational intensive routine was optimized using GPUs. 
Section 4 discusses how the results were obtained. Section 5 presents and discusses the results. Finally, 
Section 6 provides an overview of our future work.

\section{Boundary Elements Method Background}

Without addressing details on BEM formulation, the Boundary Integral Equation for Stationary
Elastodynamic Problems can be written as:

$\vspace{-1em}$
\begin{equation}
	c_{ij}u_{j}(\xi,\omega) + \int_S t_{ij}^*(\xi, x, \omega)u_j (x, \omega)\text{d}S(x) = \int_S u_{ij}^*(\xi, x, \omega) t_j(x, \omega)\text{d}S(x) \label{bem_formulation}
\end{equation}

After performing the geometry discretization, Equation ($\ref{bem_formulation}$) can be represented in matrix form as: 
%$\vspace{-0.5em}$
\begin{equation}
	Hu = Gt \label{eqmatrix}
\end{equation}
Functions $u_{ij}^{*}(\xi, x, \omega)$ and $t_{ij}^{*}(\xi, x, \omega)$ (called fundamental solutions)
present a singular behavior when $\xi = x$ ordely $O(1/r)$, called weak singularity, and $O(1/r^2)$,
called strong singularity, respectively. The $r$ value represents the distance between $x$ and $\xi$
points. The integral of these functions, as seen in Eq. ($\ref{bem_formulation}$), will generate the $G$ and $H$ matrices respectively,
as is shown in Eq. ($\ref{eqmatrix}$).

To overcome the mentioned problem in the strong singularity, one can use the artifice known as Regularization of 
the Singular Integral, expressed as follows:
%
\begin{equation}
\begin{split}
	c_{ij}(\xi)u_{j}(\xi, \omega) + \int_{S}\left[t_{ij}^{*}(\xi, x, \omega)_{\text{DYN}} - t_{ij}^{*}(\xi, x)_{\text{STA}} \right]u_{j}(x, \omega) \text{d}S(x) + \\
	+ \int_S t_{ij}(\xi, x)_\text{STA} u_j(x)\text{d}S(x) = \int_S u_{ij}^{*}(\xi, x, \omega)_{\text{DYN}} t_j(x, \omega)\text{d}S(x) \label{singular}
\end{split}
\end{equation}
Where DYN = Dynamic, STA = Static. The integral of the difference between the dynamic and static nuclei, 
the first term in Equation ($\ref{singular}$), does not present singularity when executed concomitantly as expressed because 
they have the same order in the both problems.

Algorithmically, equation ($\ref{bem_formulation}$) is implemented into a routine named $\texttt{Nonsingd}$, computing the
integral using the Gaussian Quadrature without addressing problems related to singularity. 
To overcome singularity problems, there is a special routine called $\texttt{Sing\_de}$ that uses the
artifice described in equation ($\ref{singular}$). Lastly, $\texttt{Ghmatecd}$ is a routine 
developed to create both the $H$ and $G$ matrices described in equation ($\ref{eqmatrix}$). Both $\texttt{Nonsingd}$
and $\texttt{Sing\_de}$ are called from $\texttt{Ghmatecd}$ routine.

\section{Gaussian Quadrature Background}

Some integrals can only be approximated by numerical methods such as the Gaussian quadrature, that means:
\begin{equation}
	\int_{-1}^{1} f(x)\text{d}x \approx \sum_{j = 0}^{g}a_j f(x_j) \label{quadrature}
\end{equation}

Where $a_j$ are called weights and $x_j$ are called abscissae, and these values can be 
computed using Legendre Polynomials, as introduced below.


\begin{definition}
Legendre Polynomials are given by the following recurrence:

\begin{equation}
	  \phi_j(x)=\begin{cases}
	      1, & \text{if $j = 0$}.\\
	      x, & \text{if $j = 1$}.\\
		  \frac{2j-1}{j}x\phi_{j-1}(x) - \frac{j-1}{j}\phi_{j-2}(x) & \text{if $j \in \mathbb{N} - \{0, 1\}$}
	  \end{cases}
\end{equation}
\end{definition}

It can be shown that those polynomials have the following properties:

\begin{theorem}
Legendre Polynomials satisfy the following properties \citep{ascher:2011}: 
\begin{enumerate}
	\item Orthogonality: for $i \neq j$, $\int_{-1}^{1} \phi_{i}(x)\phi_{j}(x) \text{d}x = 0$.
	\item Calibration: $|\phi_j(x)| \leq 1$ for any $-1 \leq x \leq 1$, and $\phi_j(1) = 1$.
	\item Oscillation: $\phi_j(x)$ has degree equal to $j$ and all its roots are inside $]-1; 1[$.
\end{enumerate}
\end{theorem}
The proof of such theorem is beyond the scope of this work. See \citep{ascher:2011}.

\begin{theorem}
	Let $p(x)$ be a polynomial of degree less but not equal to $j$. Then $p(x)$ is orthogonal to $\phi_j(x)$, that is:
	\begin{equation}
		\int_{-1}^{1} p(x)\phi_j(x) \text{d}x = 0
	\end{equation}
\end{theorem}
\begin{proof}
	Since $\{\phi_0, \phi_1, \cdots, \phi_j\}$ is an orthogonal base of all polynomials of degree less or 
	equal $j$, then all polynomials less or equal to $j$ can be written as linear combination of 
	$\phi_0, \phi_1, \cdots, \phi_j$. This also applies to $p(x)$, since it has degree less but not 
	equal than $j$. That means:

	\begin{equation}
		p(x) = \sum_{k = 0}^{j-1} \alpha_k\phi_k(x)
	\end{equation}
	with such information, just calculate:
	\begin{equation}
		\int_{-1}^{1} p(x)\phi_j(x)\text{d}x = \int_{-1}^{1} \left(\sum_{k = 0}^{j-1} \alpha_k \phi_k(x) \right)\phi_j(x)
           \text{d}x  = \sum_{k = 0}^{j-1} \alpha_k \left(\underbrace{\int_{-1}^{1} \phi_k(x)\phi_j(x)\text{d}x}_{0, \text{orthogonality}} \right) = 0
	\end{equation}
\end{proof}

\section{LU Decomposition Background}

In many areas of science concerning numerical methods, it is necessary to find a solution that 
satisfies together many equations. In this subsection, we describe one of the 
most used algorithms to solve a specific kind of linear system of equations. 
Before showing such algorithms, a set of definitions and theorems are required 
to understand how it operates.

A \textit{matrix} is denoted as an element of $\Cfield^{m \times n}$, where
$m$ is the number of rows and $n$ is the number of columns. A \textit{vector} 
is an element of $\Cfield^m$, where $m$ is the number of rows. Notice that a 
vector is a single column matrix.

\begin{definition}
A system of linear equations is a equation of the form $Ax = b$, where 
$A \in \Cfield^{m \times n}, b \in \Cfield^{n}$ are known and $x \in \Cfield^{m}$
is the only unknown in the equation.
\end{definition}

Although the definition above is general to any linear system, 
here we will explore properties of linear systems characterized by a square and
nonsingular matrix $A$.

\begin{definition}
	A \textbf{square} matrix is such that the number of rows is equal to
	the number of columns. A matrix that is not square is called \textbf{rectangular}.
\end{definition}

\begin{definition}
	A matrix $A \in \Cfield^{n \times n}$ is called nonsingular if and only if
	$Ax = 0 \Leftrightarrow x = 0$.
\end{definition}

Linear systems that have nonsingular matrices have a unique solution, as demonstrated below.

\begin{theorem}
	Let $A \in \Cfield^{n \times n}$ be a nonsingular square matrix. Then the system
	$Ax = b$ admits a unique solution $x \in \Cfield^{n}$.
\end{theorem}
\begin{proof}
	Let $A$ be a nonsingular square matrix and suppose, by absurd, that $Ax = b$ have two distinct 
	solutions named $x$ and $y$. Since $y$ is also a solution, then $Ay = b$. But then $Ax - Ay = b - b = 0$. 
	Isolating $A$ we find that $A(x - y) = 0$. But since $A$ is nonsingular, then by definition of nonsingularity we 
	have that $(x - y) = 0$, implying that $x = y$. This result is an absurd because we supposed that $x$ and $y$ 
	are distinct solutions.
\end{proof}

There is also an interesting special case of square matrices, called triangular matrices. There are two types of 
triangular matrices, lower triangular and upper triangular.

\begin{definition}
An upper triangular matrix is such that all elements below the main diagonal are $0$. Analogously, a lower 
triangular matrix is such that all elements above the main diagonal are $0$.
\end{definition}

The matrices below are examples of triangular matrices. At the left, we have an upper triangular matrix. 
At the right, we have a lower triangular matrix.

\[ \begin{pmatrix}
  1 & 2 & 3 \\
  0 & 4 & 5 \\
  0 & 0 & 6
		\end{pmatrix} \qquad
 \begin{pmatrix}
  1 & 0 & 0 \\
  2 & 3 & 0 \\
  4 & 5 & 6
		\end{pmatrix}
\]

Systems of equations with triangular matrices have an interesting property that it can be solved with an $O(n²)$ algorithm, as 
illustrated in algorithms \ref{forward} and \ref{backward}.


\begin{algorithm}[H]
\caption{Solves $Ax = b$, where $A$ is a lower triangular nonsingular matrix. Replaces b with the result }
\label{forward}
\begin{algorithmic}[1]
    \Procedure{forward\_substituition}{$A \in \Cfield^{n \times n}$, $b \in \Cfield^{n}$}
		\For{$j := 1, n$}
			\If{$A[j][j] == 0$}
				\State{Error: $A$ is singular.}
			\EndIf
			\State{$b[j] \leftarrow b[j]/A[j][j]$}

            \For{$i := j+1, n$}
                \State{$b[i] \leftarrow b[i] - A[i][j]*b[j]$}
            \EndFor
     \EndFor
    \EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Solves $Ax = b$, where $A$ is a upper triangular nonsingular matrix. Replaces b with the result}
\label{backward}
\begin{algorithmic}[1]
    \Procedure{backward\_substituition}{$A \in \Cfield^{n \times n}$, $b \in \Cfield^{n}$}
		\For{$j := n, 1$}
			\If{$A[j][j] == 0$}
				\State{Error: $A$ is singular.}
			\EndIf
			\State{$b[j] \leftarrow b[j]/A[j][j]$}
            \For{$i := 1, j$}
                \State{$b[i] \leftarrow b[i] - A[i][j]*b[j]$}
            \EndFor
     \EndFor
    \EndProcedure
\end{algorithmic}
\end{algorithm}

The question that rises now is now is how can we reduce a square nonsingular matrix $A$ to triangular matrices. 
This is what LU with partial pivoting does, it decomposes $A$ in three matrices $P^{\intercal}LU$, where $P$ is a pivoting matrix, 
$L$ is lower triangular and $U$ is upper triangular. Briefly, a pivoting matrix is such that when operated with a matrix, 
it interchanges its columns; this is used to avoid divisions by $0$ \citep{watkins:2004}. Algorithm $\ref{lu}$ illustrates how $A$ can be decomposed
in such matrices.

\begin{algorithm}[H]
\caption{Decomposes $A$ in $P^{\intercal}LU$, $P$ is a permutation matrix stored in a vector. Stores $L$ and $U$ over $A$.}
\label{lu}
\begin{algorithmic}[1]
    \Procedure{lu\_partial\_pivoting}{$A \in \Cfield^{n \times n}$}
		\State{Allocate $P \in \mathbb{N}^{n-1}$}
		\For{$k := 1, n-1$}
			\State{$amax \leftarrow \text{max}\{|A[k][k]|, |A[k+1][k]|, \cdots, |A[n][k]|\} $}
			\If{$amax == 0$}
				\State{Error: $A$ is singular.}
			\EndIf
			\State{$m \leftarrow$ smaller integer $\geq k$ that $|A[m][k]| == amax$}
			\State{$P[k] \leftarrow m$}
			\If{$m \neq k$}
				\State{Swap column $m$ and $k$}
			\EndIf
            \For{$i := k+1, n$}
                \State{$A[i][k] \leftarrow A[i][k]/A[k][k]$}
            \EndFor
            \For{$j := k+1, n$}
				\For{$i := k+1, n$}
					\State{$A[i][j] \leftarrow A[i][j] - A[i][k]*A[k][j]$}
				\EndFor
            \EndFor
     \EndFor
	\If{$A[n][n] == 0$}
		\State{Error: $A$ is singular.}
	\EndIf
    \EndProcedure
\end{algorithmic}
\end{algorithm}

Using the fact that $A$ = $P^{\intercal}LU$, one can solve $Ax=b$ by solving $P^{\intercal}LUx = b$. Since $P$ is a permutation matrix, then its inverse $P^{-1} = P^{\intercal}$ 
\citep{watkins:2004} and now one must solve $LUx = Pb$. Let $y = Ux$. Solving $Ly = Pb$ will result in a numerical value to $y$. Solving $Ux = y$ will finally 
assert $x$. 

Since the cost of decomposing $A$ into $P^{\intercal}LU$ is $O(n³)$, the time required to compute $P^{\intercal}b$ is, naively, $O(n²)$ and the time required to solve
the two triangular systems is $O(n²)$, then the cost of solving $Ax=b$ is $O(n³)$. 

%\begin{theorem}
%Let A be a square nonsingular matrix. Then the LU decomposition with partial pivoting decomposes $A$ in three matrices such that $A = PLU$, 
%where P is a permutation matrix.
%\end{theorem}
%\begin{proof}
%	See Theorem 1.8.8 of \cite{watkins:2004}
%\end{proof}




\section{CUDA Programming Background}

Uma monografia deve ter um capítulo inicial que é a Introdução e um
capítulo final que é a Conclusão. Entre esses dois capítulos poderá
ter uma sequência de capítulos que descrevem o trabalho em detalhes.
Após o capítulo de conclusão, poderá ter apêndices e ao final deverá
ter as referências bibliográficas.


Para a escrita de textos em Ciência da Computação, o livro de Justin Zobel, 
\emph{Writing for Computer Science} \citep{zobel:04} é uma leitura obrigatória. 
O livro \emph{Metodologia de Pesquisa para Ciência da Computação} de 
\citet{waz:09} também merece uma boa lida.

O uso desnecessário de termos em lingua estrangeira deve ser evitado. No entanto,
quando isso for necessário, os termos devem aparecer \emph{em itálico}.

\begin{small}
\begin{verbatim}
Modos de citação:
indesejável: [AF83] introduziu o algoritmo ótimo.
indesejável: (Andrew e Foster, 1983) introduziram o algoritmo ótimo.
certo : Andrew e Foster introduziram o algoritmo ótimo [AF83].
certo : Andrew e Foster introduziram o algoritmo ótimo (Andrew e Foster, 1983).
certo : Andrew e Foster (1983) introduziram o algoritmo ótimo.
\end{verbatim}
\end{small}

Uma prática recomendável na escrita de textos é descrever as legendas das
figuras e tabelas em forma auto-contida: as legendas devem ser razoavelmente
completas, de modo que o leitor possa entender a figura sem ler o texto onde a
figura ou tabela é citada.  

Apresentar os resultados de forma simples, clara e completa é uma tarefa que
requer inspiração. Nesse sentido, o livro de \citet{tufte01:visualDisplay},
\emph{The Visual Display of Quantitative Information}, serve de ajuda na
criação de figuras que permitam entender e interpretar dados/resultados de forma
eficiente.

