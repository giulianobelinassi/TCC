%% ------------------------------------------------------------------------- %%
\chapter{Introdução}
\label{cap:introducao}


Differential equations governing problems of Mathematical Physics have analytical 
solutions only in cases in which the domain geometry, boundary and initial conditions are
reasonably simple. Problems with arbitrary domains and fairly general boundary conditions 
can only be solved approximately, for example, by using numerical techniques. 
These techniques were strongly developed due to the presence of increasingly powerful computers, 
enabling the solution of complex mathematical problems.

The Boundary Element Method (BEM) is a very efficient alternative for modeling unlimited domains since
it satisfies the Sommerfeld radiation condition, also known as geometric damping 
\cite{katsikadelis:2016}. This method can be used for numerically modeling the stationary behavior of 3D 
wave propagation in the soil and it is useful as a computational tool to aid in the analysis of soil vibration
\cite{dominguez:1993}. A BEM based tool can be used for analyzing the vibration created 
by heavy machines, railway lines, earthquakes, or even to aid the design of offshore oil platforms.

With the advent of GPUs, several mathematical and engineering simulation problems were redesigned
to be implemented into these massively parallel devices. However, 
first GPUs were designed to render graphics in real time, as a consequence, all the available 
libraries, such as OpenGL, were graphical oriented. These redesigns involved converting 
the original problem to the graphics domain and required expert knowledge of the selected 
graphical library. 

NVIDIA noticed a new demand for their products and created an API called CUDA to enable 
the use of GPUs for general purpose programming. CUDA uses the concept of kernels, which are
functions called from the host to be executed by GPU threads. Kernels are organized into a set of blocks composed of a set 
of threads that cooperate with each other \cite{patterson:2007}.

The memory of a NVIDIA GPU is divided in global memory, local memory, and shared memory.
Global memory is accessible by all threads, local memory is private to a thread and shared 
memory is low-latency and accessible by all threads in a block\cite{patterson:2007}. CUDA 
provides mechanisms to access all of them.

Regarding this work, this parallelization approach is useful because an analysis of a large domain 
requires a proportionally large number of mesh elements, and processing a single element have a high
time cost. Doing such analysis in parallel reduces the computational time requied for the entire
program because multiple elements are processed at the same time. This advantage was provided by this
research.

\textbf{Talk about the structure of this paper}.

Before discussing any parallelization technique or results, Section 2 presents a very brief mathematical 
description of BEM for Stationary Elastodynamic Problems and the meaning of some functions presented in 
this paper. Section 3 shows how the most computational intensive routine was optimized using GPUs. 
Section 4 discusses how the results were obtained. Section 5 presents and discusses the results. Finally, 
Section 6 provides an overview of our future work.

\section{Boundary Elements Method Background}

Without addressing details on BEM formulation, the Boundary Integral Equation for Stationary
Elastodynamic Problems can be written as:

$\vspace{-1em}$
\begin{equation}
	c_{ij}u_{j}(\xi,\omega) + \int_S t_{ij}^*(\xi, x, \omega)u_j (x, \omega)\text{d}S(x) = \int_S u_{ij}^*(\xi, x, \omega) t_j(x, \omega)\text{d}S(x) \label{bem_formulation}
\end{equation}

After performing the geometry discretization, Equation ($\ref{bem_formulation}$) can be represented in matrix form as: 
%$\vspace{-0.5em}$
\begin{equation}
	Hu = Gt \label{eqmatrix}
\end{equation}
Functions $u_{ij}^{*}(\xi, x, \omega)$ and $t_{ij}^{*}(\xi, x, \omega)$ (called fundamental solutions)
present a singular behavior when $\xi = x$ ordely $O(1/r)$, called weak singularity, and $O(1/r^2)$,
called strong singularity, respectively. The $r$ value represents the distance between $x$ and $\xi$
points. The integral of these functions, as seen in Eq. ($\ref{bem_formulation}$), will generate the $G$ and $H$ matrices respectively,
as is shown in Eq. ($\ref{eqmatrix}$).

To overcome the mentioned problem in the strong singularity, one can use the artifice known as Regularization of 
the Singular Integral, expressed as follows:
%
\begin{equation}
\begin{split}
	c_{ij}(\xi)u_{j}(\xi, \omega) + \int_{S}\left[t_{ij}^{*}(\xi, x, \omega)_{\text{DYN}} - t_{ij}^{*}(\xi, x)_{\text{STA}} \right]u_{j}(x, \omega) \text{d}S(x) + \\
	+ \int_S t_{ij}(\xi, x)_\text{STA} u_j(x)\text{d}S(x) = \int_S u_{ij}^{*}(\xi, x, \omega)_{\text{DYN}} t_j(x, \omega)\text{d}S(x) \label{singular}
\end{split}
\end{equation}
Where DYN = Dynamic, STA = Static. The integral of the difference between the dynamic and static nuclei, 
the first term in Equation ($\ref{singular}$), does not present singularity when executed concomitantly as expressed because 
they have the same order in the both problems.

Algorithmically, equation ($\ref{bem_formulation}$) is implemented into a routine named $\texttt{Nonsingd}$, computing the
integral using the Gaussian Quadrature without addressing problems related to singularity. 
To overcome singularity problems, there is a special routine called $\texttt{Sing\_de}$ that uses the
artifice described in equation ($\ref{singular}$). Lastly, $\texttt{Ghmatecd}$ is a routine 
developed to create both the $H$ and $G$ matrices described in equation ($\ref{eqmatrix}$). Both $\texttt{Nonsingd}$
and $\texttt{Sing\_de}$ are called from $\texttt{Ghmatecd}$ routine.

\section{Gaussian Quadrature Background}

Some integrals can only be approximated by numerical methods such as the Gaussian quadrature, that means:
\begin{equation}
	\int_{-1}^{1} f(x)\text{d}x \approx \sum_{j = 1}^{n}a_j f(x_j) \label{quadrature}
\end{equation}

Where $a_j$ are called weights and $x_j$ are called abscissae, and these values can be 
computed using Legendre Polynomials, as introduced below.



\begin{definition}
Legendre Polynomials are given by the following recurrence:

\begin{equation}
	  \phi_j(x)=\begin{cases}
	      1, & \text{if $j = 0$}.\\
	      x, & \text{if $j = 1$}.\\
		  \frac{2j-1}{j}x\phi_{j-1}(x) - \frac{j-1}{j}\phi_{j-2}(x) & \text{if $j \in \mathbb{N} - \{0, 1\}$}
	  \end{cases}
\end{equation}
\end{definition}

It can be shown that those polynomials have the following important properties:

\begin{theorem}
Legendre Polynomials satisfy the following properties \citep{ascher:2011}: 
\begin{enumerate}
	\item Orthogonality: for $i \neq j$, $\int_{-1}^{1} \phi_{i}(x)\phi_{j}(x) \text{d}x = 0$.
	\item Calibration: $|\phi_j(x)| \leq 1$ for any $-1 \leq x \leq 1$, and $\phi_j(1) = 1$.
	\item Oscillation: $\phi_j(x)$ has degree equal to $j$ and all its roots are inside $]-1; 1[$.
\end{enumerate}
\end{theorem}
The proof of such theorem is beyond the scope of this work. See \citep{ascher:2011}.

\begin{theorem} \label{ortho}
	Let $q(x)$ be a polynomial of degree $< n$. Then $q(x)$ is orthogonal to $\phi_n(x)$, that is:
	\begin{equation}
		\int_{-1}^{1} q(x)\phi_n(x) \text{d}x = 0
	\end{equation}
\end{theorem}
\begin{proof}
	Since $\{\phi_0, \phi_1, \cdots, \phi_n\}$ is an orthogonal base of all polynomials of degree $\leq n$, 
	then all polynomials of degree $\leq n$ can be written as linear combination of 
	$\phi_0, \phi_1, \cdots, \phi_n$. Since $q(x)$ degree is $< n$, then:
	\begin{equation}
		q(x) = \sum_{k = 0}^{n-1} \alpha_k\phi_k(x)
	\end{equation}
	with such information, just calculate:
	\begin{equation}
		\int_{-1}^{1} q(x)\phi_n(x)\text{d}x = \int_{-1}^{1} \left(\sum_{k = 0}^{n-1} \alpha_k \phi_k(x) \right)\phi_n(x)
           \text{d}x  = \sum_{k = 0}^{n-1} \alpha_k \left(\underbrace{\int_{-1}^{1} \phi_k(x)\phi_n(x)\text{d}x}_{0, \text{orthogonality}} \right) = 0
	\end{equation}
\end{proof}

These two properties above are essential to capture the main concept of the Gaussian quadrature, which is to satisfy the equation below with equality.
Let $r(x)$ be a polynomial of degree $< n$.
\begin{equation}
	\int_{-1}^{1} r(x) \text{d}x = \sum_{j = 1}^{n} a_jr(x_j)
\end{equation}

Let's now show a simple trick that can be done with Legendre Polynomials to enhance the quadrature precision. Let $p(x)$ be a polynomial of degree $< 2n$. 
If we divide $p(x)$ by $\phi_n(x)$, both quotient $q(x)$ and the remainder $r(x)$ have degree $< n$ because $\phi_n(x)$ have degree equal to $n$. 
That means:
\begin{equation}
	p(x) = q(x)\phi_n(x) + r(x)
\end{equation}
Integrating both sides:
\begin{equation}
	\int_{-1}^{1} p(x)\text{d}x = \underbrace{\int_{-1}^{1} q(x)\phi_n(x) \text{d}x}_{0, \text{by Theorem } \ref{ortho}} + \int_{-1}^{1} r(x) \text{d}x = \int_{-1}^{1} r(x) \text{d}x 
\end{equation}

Let's now select the abscissae points wisely. If all $x_j$ are zeroes of the Legendre Polynomials ($\phi_n(x_j) = 0$), then we would have:
\begin{equation}
	p(x_j) = q(x_j)\underbrace{\phi_n(x_j)}_{0} + r(x_j) = r(x_j)
\end{equation}
This means that the quadrature is exact to any polynomial of degree up to $2n-1$ if we could select the weights properly,
thus the quadrature precision would be as good as we could approximate $f(x)$ by a polynomial of degree $2n-1$. For the weight points, \cite{hildebrand:1987}
shows that one could use:
\begin{equation}
	a_j = \frac{2(1 - {x_j}^2)}{(n+1)^2 (\phi_{n+1}(x_j))^2}
\end{equation}

\section{LU Decomposition Background}



In many areas of science concerning numerical methods, it is necessary to find a solution that 
satisfies together many equations. In this subsection, we describe one of the 
most used algorithms to solve a specific kind of linear system of equations. 
Before showing such algorithms, a set of definitions and theorems are required 
to understand how it operates.

A \textit{matrix} is denoted as an element of $\Cfield^{m \times n}$, where
$m$ is the number of rows and $n$ is the number of columns. A \textit{vector} 
is an element of $\Cfield^m$, where $m$ is the number of rows. Notice that a 
vector is a single column matrix.

\begin{definition}
A system of linear equations is a equation of the form $Ax = b$, where 
$A \in \Cfield^{m \times n}, b \in \Cfield^{n}$ are known and $x \in \Cfield^{m}$
is the only unknown in the equation.
\end{definition}

Although the definition above is general to any linear system, 
here we will explore properties of linear systems characterized by a square and
nonsingular matrix $A$.

\begin{definition}
	A \textbf{square} matrix is such that the number of rows is equal to
	the number of columns. A matrix that is not square is called \textbf{rectangular}.
\end{definition}

\begin{definition}
	A matrix $A \in \Cfield^{n \times n}$ is called nonsingular if and only if
	$Ax = 0 \Leftrightarrow x = 0$.
\end{definition}

Linear systems that have nonsingular matrices have a unique solution, as demonstrated below.

\begin{theorem}
	Let $A \in \Cfield^{n \times n}$ be a nonsingular square matrix. Then the system
	$Ax = b$ admits a unique solution $x \in \Cfield^{n}$.
\end{theorem}
\begin{proof}
	Let $A$ be a nonsingular square matrix and suppose, by absurd, that $Ax = b$ have two distinct 
	solutions named $x$ and $y$. Since $y$ is also a solution, then $Ay = b$. But then $Ax - Ay = b - b = 0$. 
	Isolating $A$ we find that $A(x - y) = 0$. But since $A$ is nonsingular, then by definition of nonsingularity we 
	have that $(x - y) = 0$, implying that $x = y$. This result is an absurd because we supposed that $x$ and $y$ 
	are distinct solutions.
\end{proof}

There is also an interesting special case of square matrices, called triangular matrices. There are two types of 
triangular matrices, lower triangular and upper triangular.

\begin{definition}
An upper triangular matrix is such that all elements below the main diagonal are $0$. Analogously, a lower 
triangular matrix is such that all elements above the main diagonal are $0$.
\end{definition}

The matrices below are examples of triangular matrices. At the left, we have an upper triangular matrix. 
At the right, we have a lower triangular matrix.

\[ \begin{pmatrix}
  1 & 2 & 3 \\
  0 & 4 & 5 \\
  0 & 0 & 6
		\end{pmatrix} \qquad
 \begin{pmatrix}
  1 & 0 & 0 \\
  2 & 3 & 0 \\
  4 & 5 & 6
		\end{pmatrix}
\]

Systems of equations with triangular matrices have an interesting property that it can be solved with an $O(n²)$ algorithm, as 
illustrated in algorithms \ref{forward} and \ref{backward}.


\begin{algorithm}[H]
\caption{Solves $Ax = b$, where $A$ is a lower triangular nonsingular matrix. Replaces b with the result }
\label{forward}
\begin{algorithmic}[1]
    \Procedure{forward\_substituition}{$A \in \Cfield^{n \times n}$, $b \in \Cfield^{n}$}
		\For{$j := 1, n$}
			\If{$A[j][j] == 0$}
				\State{Error: $A$ is singular.}
			\EndIf
			\State{$b[j] \leftarrow b[j]/A[j][j]$}

            \For{$i := j+1, n$}
                \State{$b[i] \leftarrow b[i] - A[i][j]*b[j]$}
            \EndFor
     \EndFor
    \EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Solves $Ax = b$, where $A$ is a upper triangular nonsingular matrix. Replaces b with the result}
\label{backward}
\begin{algorithmic}[1]
    \Procedure{backward\_substituition}{$A \in \Cfield^{n \times n}$, $b \in \Cfield^{n}$}
		\For{$j := n, 1$}
			\If{$A[j][j] == 0$}
				\State{Error: $A$ is singular.}
			\EndIf
			\State{$b[j] \leftarrow b[j]/A[j][j]$}
            \For{$i := 1, j$}
                \State{$b[i] \leftarrow b[i] - A[i][j]*b[j]$}
            \EndFor
     \EndFor
    \EndProcedure
\end{algorithmic}
\end{algorithm}

The question that rises now is now is how can we reduce a square nonsingular matrix $A$ to triangular matrices. 
This is what LU with partial pivoting does, it decomposes $A$ in three matrices $P^{\intercal}LU$, where $P$ is a pivoting matrix, 
$L$ is lower triangular and $U$ is upper triangular. Briefly, a pivoting matrix is such that when operated with a matrix, 
it interchanges its columns; this is used to avoid divisions by $0$ \citep{watkins:2004}. Algorithm $\ref{lu}$ illustrates how $A$ can be decomposed
in such matrices.

\begin{algorithm}[H]
\caption{Decomposes $A$ in $P^{\intercal}LU$, $P$ is a permutation matrix stored in a vector. Stores $L$ and $U$ over $A$.}
\label{lu}
\begin{algorithmic}[1]
    \Procedure{lu\_partial\_pivoting}{$A \in \Cfield^{n \times n}$}
		\State{Allocate $P \in \mathbb{N}^{n-1}$}
		\For{$k := 1, n-1$}
			\State{$amax \leftarrow \text{max}\{|A[k][k]|, |A[k+1][k]|, \cdots, |A[n][k]|\} $}
			\If{$amax == 0$}
				\State{Error: $A$ is singular.}
			\EndIf
			\State{$m \leftarrow$ smaller integer $\geq k$ that $|A[m][k]| == amax$}
			\State{$P[k] \leftarrow m$}
			\If{$m \neq k$}
				\State{Swap column $m$ and $k$}
			\EndIf
            \For{$i := k+1, n$}
                \State{$A[i][k] \leftarrow A[i][k]/A[k][k]$}
            \EndFor
            \For{$j := k+1, n$}
				\For{$i := k+1, n$}
					\State{$A[i][j] \leftarrow A[i][j] - A[i][k]*A[k][j]$}
				\EndFor
            \EndFor
     \EndFor
	\If{$A[n][n] == 0$}
		\State{Error: $A$ is singular.}
	\EndIf
    \EndProcedure
\end{algorithmic}
\end{algorithm}

Using the fact that $A$ = $P^{\intercal}LU$, one can solve $Ax=b$ by solving $P^{\intercal}LUx = b$. Since $P$ is a permutation matrix, then its inverse $P^{-1} = P^{\intercal}$ 
\citep{watkins:2004} and now one must solve $LUx = Pb$. Let $y = Ux$. Solving $Ly = Pb$ will result in a numerical value to $y$. Solving $Ux = y$ will finally 
assert $x$. 

Since the cost of decomposing $A$ into $P^{\intercal}LU$ is $O(n³)$, the time required to compute $P^{\intercal}b$ is, naively, $O(n²)$ and the time required to solve
the two triangular systems is $O(n²)$, then the cost of solving $Ax=b$ is $O(n³)$. 

%\begin{theorem}
%Let A be a square nonsingular matrix. Then the LU decomposition with partial pivoting decomposes $A$ in three matrices such that $A = PLU$, 
%where P is a permutation matrix.
%\end{theorem}
%\begin{proof}
%	See Theorem 1.8.8 of \cite{watkins:2004}
%\end{proof}




\section{Parallel Programming Background}

Imagine the following scenario: You have to build a bridge to connect two parts of a city, 
named A and B, that are separated by a river. Let's say that if a single person builds 
this bridge from A to B, the time required to do so is $t$. How can we build this bridge 
faster? If we have another man building the same bridge in parallel to you but from B to A
and connect it at the middle of the river, then the time required is $t/2$. 
This silly example captures the essence of parallel computing. How can we use multiple 
processors or multiple machines with some coordination to archive the same purpose?

An important abstraction that makes parallel computing easier to understand is the concept 
of processes and threads. A process is an abstraction layer that allows a program to have 
the perception that it is running alone on a computer, although the process is controlled 
by the operating system such as Windows. Every process has its own resources and multiple 
execution units \cite{tanenbaum:2009}. A single execution unit is called a thread.

There are various computer architectures designed to handle parallel computing, as described 
by Flynn Taxonomy \citep{pacheco:2011}, but remarks are necessary to two architectures.

\begin {enumerate}
	\item SIMD: Standing for Single Instruction, Multiple Data. This refers to vectorized processors allowing 
		a single operation to be executed in a vector content. Examples are Intel SSE and GPUs, although GPUs 
		are not a pure SIMD, as described later.

	\item MIMD: Stands for Multiple Instruction, Multiple Data. This refers to independent multicore systems,
		capable of executing tasks asynchronously.
\end{enumerate}

Shared memory systems are examples of MIND that the processors read and write to a global shared memory. 
For such systems, a collection of directives called OpenMP can be used to explore its parallel capabilities.

