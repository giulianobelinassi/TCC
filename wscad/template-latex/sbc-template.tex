\documentclass[12pt]{article}

\usepackage{sbc-template}

\usepackage{graphicx,url}

%\usepackage[brazil]{babel}   
\usepackage[latin1]{inputenc}  

\usepackage{amsmath}
\usepackage{amssymb} 
\usepackage{mathtools}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

 
\newcommand{\Cfield}{\mathbb{C}}
\newcommand{\Rfield}{\mathbb{R}}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\sloppy

\title{Optimizing a Boundary Elements Method implementations with GPU}

\author{Giuliano A. F. Belinassi\inst{1}, Rodrigo Siqueira\inst{1}, Ronaldo Carrion\inst{2}
  Alfredo Goldman\inst{1} }


\address{Instituto de Matemática e Estatística (IME) -- Universidade de São Paulo
  (USP)\\
  Rua do Matão, 1010 -- São Paulo -- SP -- Brazil
\nextinstitute
  Escola Politécnica (EP)  -- Universidade de São Paulo\\
  Avenue Professor Mello Moraes, 2603 -- São Paulo -- SP -- Brazil
}

\begin{document} 

\maketitle

\begin{abstract}
  This meta-paper describes the style to be used in articles and short papers
  for SBC conferences. For papers in English, you should add just an abstract
  while for the papers in Portuguese, we also ask for an abstract in
  Portuguese (``resumo''). In both cases, abstracts should not have more than
  10 lines and must be in the first page of the paper.
\end{abstract}
     
\section{Introduction}

Since the computer was proven to be a useful machine, there always has been an interest of 
building faster versions of it to solve bigger and more complex problems in a matter 
of time that no human can. One way archiving this is by building more complex sequential 
CPUs, with more transistors. Recently, the size of CPU components got so small that it became
difficult to create such faster sequential CPUs \cite{brock:2006}. As a consequence of this fact, 
CPU manufacturers are investing in multicore CPUs, capable of running more than one task 
asynchronously.

Parallel computing is hard to define, but intuitively, it is a computation method 
that allows data to be distributed and processed simultaneously. In Flynn's taxonomy
\cite{pacheco:2011},
there are two types of parallel computer archictectures: 

\begin{enumerate}
\item Single Instruction, Multiple Data (SIMD); a processor that allows a chunk of
data to be loaded and a single instruction to be used to process it. As example of SIMD,
there is Intel's SSE \cite{sse} and Graphics Processing Units (GPU) are examples it. 

\item Multiple Instruction, Multiple Data (MIMD); a system consisting of
multiple independent processing units, executing asynchronously. Multicore CPUs are a 
example of MIMD.
\end{enumerate}

Originally, GPU was designed to render graphics in real time and OpenGL and DirectX 
were the first libraries designed to access GPUs resources and provide graphics. 
However, researchers realized that GPUs could be used for other applications different 
from graphics. Consequently, these libraries were used by engineers and scientists 
in their specific problems, however, they had to convert their problem into a graphical 
domain.

NVIDIA noticed a new demand for their products and created an API called CUDA to enable 
the use of GPUs in general purpose situation. CUDA has the concept of kernels, which are
 functions called from host to be executed in 
the GPU threads. Kernels are organized into a set of blocks wherein each block is a set 
of threads that cooperate with each other \cite{patterson:2007}.

GPU's memory is divided into global memory, local memory, and shared memory. First, it is 
a memory that all threads can access. Second, it is a memory that is private to a thread. 
Third, it is a  low-latency memory that is shared between all threads in the same block.
\cite{patterson:2007}.


\section{Context}

Differential equations governing problems of Mathematical Physics only have analytical 
solutions in cases in which both the domain geometry and the boundary and initial conditions 
are reasonably simple. Problems with arbitrary domains and fairly general boundary conditions 
can only be solved in an approximate way, for example, by numerical techniques. These techniques 
have experienced strong development due to the presence of increasingly powerful digital electronic 
computers, allowing the solution of complex mathematical problems. The Boundary Element Method (BEM) 
is a very efficient alternative for modeling unlimited domains, since it naturally satisfies the 
Sommerfeld radiation condition, also known as geometric damping.
Such method can be used to numerically model the 3D stationary behavior of wave propagation in the soil 
based on BEM with the objective of creating a computational tool which can assist in analysis such as 
evaluation of vibrations in the soil rising from machine tools operation or railway lines as well as 
to understand the role of soil during earthquakes or even in the design of offshore oil platforms.

Wihout addressing details on the BEM formulation, the Bondary Integral Equation for Stationary
Elastodynamic Problems can be written as:

\begin{equation}
	c_{ij}u_{j}(\xi,\omega) + \int_S t_{ij}^*(\xi, x, \omega)u_j (x, \omega)\text{d}S(x) = \int_S u_{ij}^*(\xi, x, \omega) t_j(x, \omega)\text{d}S(x)
\end{equation}

After performing the geometry discretization, Equation (1) can still be represented in matrix form as:

\begin{equation}
	[H]\{u\} = [G]\{t\}
\end{equation}

Algorithmically, equation (1) is implemented into a routine named $\texttt{Nonsingd}$, computing the
integral using the Gaussian Quadrature without addressing problems related to singularity. 
To overcome singularity problems, there is a special routine called $\texttt{Sigmaec}$ that uses an 
artifice known as Regularization of the Singular Integral. By last, $\texttt{Ghmatecd}$ is a routine 
developed to create both $H$ and $G$ matrices described in equation (2).

%Functions $u_{ij}^{*}(\xi, x, \omega)$ and $t_{ij}^{*}(\xi, x, \omega)$ (called fundamental solutions)
%present a singular behavior when $\xi = x$ ordely $O(1/r)$, called weak singularity, and $O(1/r^2)$,
%called strong singularity, respectively. The $r$ value represents the distance between $x$ and $\xi$
%points.
%To overcome this problem in the strong singularity, one use the artifice known as Regularization of 
%the Singular Integral that can be expressed as follows:
%
%\begin{equation}
%\begin{split}
%	c_{ij}(\xi)u_{j}(\xi, \omega) + \int_{S}\left[t_{ij}^{*}(\xi, x, \omega)_{\text{DYN}} - t_{ij}^{*}(\xi, x)_{\text{STA}} \right]u_{j}(x, \omega) \text{d}S(x) + \\
%	+ \int_S t_{ij}(\xi, x)_\text{STA} u_j(x)\text{d}S(x) = \int_S u_{ij}^{*}(\xi, x, \omega)_{\text{DYN}} t_j(x, \omega)\text{d}S(x)	
%\end{split}
%\end{equation}
%DYN = Dynamic, EST = Static. The integral of the difference between the dynamic and static nuclei, 
%first one in Equation (3), does not present singularity when executed concomitantly as expressed because 
%they have the same order in the both problems.



%Let now $\hat{x} \in \Cfield^n$
%be a value that should have been $x$, but contains errors due to whatever
%reasons. One can measure \textit{how far} $\hat{x}$ is from $x$ by computing
%$\norm{x - \hat{x}}$. If the vector $\infty$-norm is used, then the result yielded
%is the maximum difference between a single entry of the vector. If the matrix
%$\infty$-norm is used, then the result is the biggest sum of errors of a matrix
%row. The matrix 1-norm yields the biggest sum of erros of a matrix column.

\section{Parallelization Strategies}

A parallel implementation of BEM began by analyzing and modifying a sequential code 
provided by \cite{carrion:02}. Gprof \cite{binutils}, a profiling tool by GNU, showed that the most time-consuming 
routine was Nonsingd. Since most calls to $\texttt{Nonsingd}$ were from $\texttt{Ghmatecd}$, most of the parallelization effort 
was focused on that last routine.

\subsection{Ghmatecd Parallelization}

Algorithm 1 shows the pseudocode of $\texttt{Ghmatecd}$ subroutine. Let $n$ be the number of mesh elements and $m$ the number of 
boundary elements. $\texttt{Ghmatecd}$ builds matrices $H$ and $G$ by computing smaller $3\times3$ matrices returned 
by $\texttt{Nonsingd}$ and $\texttt{Sigmaec}$.

\begin{algorithm}
\caption{Creates $H, G \in \Cfield^{(3m)\times(3n)}$}\label{euclid}
\begin{algorithmic}[1]
	\Procedure{Ghmatecd}{}
		\For{$j := 1, n$} 
			\For{$i := 1, m$}
				\State{$ii := 3(i-1) + 1;     jj := 3(j-1) + 1$}
				\If{$i == j$}
					\State{$Gelement, Helement \leftarrow \text{Sigmaec}(i)$}\Comment{two $3\times3$ complex matrices}					
				\Else
					\State{$Gelement, Helement \leftarrow \text{Nonsingd}(i, j)$}	
				\EndIf
				\State{$G[ii:ii+2][jj:jj+2] \leftarrow Gelement$}
				\State{$H[ii:ii+2][jj:jj+2] \leftarrow Helement$}
			\EndFor
	 \EndFor
	\EndProcedure
\end{algorithmic}
\end{algorithm}

There is no interdependency between all iterations in lines 2-3 loops, thus, all iterations can be computed 
in parallel. Since typically high-end CPUs have 8 cores, even a small number of mesh elements generate enough 
workload to use all CPUs resources if this strategy alone is used. By another hand, a GPU contain thousands 
of processors, thus even a considerable large amount of elements may not generate a workload in a way that 
it consumes all its resources. Since $\texttt{Nonsingd}$ is the cause of the high time cost of $\texttt{Ghmatecd}$, 
the main effort was to implement
an optimized version of $\texttt{Ghmatecd}$, called $\texttt{Ghmatecd\_Nonsingd}$, that only computes the $\texttt{Nonsingd}$
case in the GPU, and leave $\texttt{Sigmaec}$ to be computed in the CPU after the computation of \texttt{Ghmatecd\_Nonsingd} 
is completed. With this, a new strategy arises when also computing Nonsingd in parallel. Let $g$ be the number of Gauss quadrature 
points. The pseudocode in Algorithm 2 pictures this new strategy.

\begin{algorithm}
\caption{Creates $H, G \in \Cfield^{(3m)\times(3n)}$}\label{euclid}
\begin{algorithmic}[1]
	\Procedure{Ghmatecd\_nonsingd}{}
		\For{$j := 1, n$} 
			\For{$i := 1, m$}
				\State{$ii := 3(i-1) + 1;     jj := 3(j-1) + 1$}
				\State{Allocate \textit{Hbuffer} and \textit{Gbuffer}, buffer of matrices $3 \times 3$ of size $g^2$}
				\If{$i \neq j$}
					\For{$y := 1, g$}
						\For{$x := 1, g$}
							\State{$\textit{Hbuffer}(x, y) \leftarrow \text{GenerateMatrixH}(i, j, x, y)$}
							\State{$\textit{Gbuffer}(x, y) \leftarrow \text{GenerateMatrixG}(i, j, x, y)$}
						\EndFor
					\EndFor
				\EndIf
				\State{$Gelement \leftarrow \text{SumAllMatricesInBuffer}(\textit{Gbuffer})$} 
				\State{$Helement \leftarrow \text{SumAllMatricesInBuffer}(\textit{Hbuffer})$}
				\State{$G[ii:ii+2][jj:jj+2] \leftarrow Gelement$}
				\State{$H[ii:ii+2][jj:jj+2] \leftarrow Helement$}
			\EndFor
	 \EndFor
	\EndProcedure

	\Procedure{Ghmatecd\_Sigmaec}{}
		\For{$i := 1, m$}
			\State{$ii := 3(i-1) + 1$}
			\State{$Gelement, Helement \leftarrow \text{Sigmaec}(i)$}	
			\State{$G[ii:ii+2][ii:ii+2] \leftarrow Gelement$}
			\State{$H[ii:ii+2][ii:ii+2] \leftarrow Helement$}
	 \EndFor
	\EndProcedure
	\Procedure{Ghmatecd}{}
		\State{$\text{Ghmatecd\_Nonsingd}()$}
		\State{$\text{Ghmatecd\_Sigmaec}()$}
	\EndProcedure
		
\end{algorithmic}
\end{algorithm}

The $\texttt{Ghmatecd\_Nonsingd}$ can be implemented as a CUDA kernel. Inside of a CUDA block, create $g \times g$ 
threads to compute in parallel the two nested loops in lines 2-3 and allocate spaces in the shared 
memory to keep the buffer of matrices ($\texttt{Hbuffer}$ and $\texttt{Gbuffer}$). Since these buffers contain matrices of size 
$3 \times 3$, nine of these $g \times g$ threads can be used to 
sum all matrices because one thread can be assigned to each matrix entry, unless $g < 3$. Notice that $g$ is also upper-bounded by the 
amount of shared memory available in the GPU. Launching $m \times n$ blocks to cover the two nested loops in lines
2 to 3 will generate the entire $H$ and $G$ without the singular part. The $\texttt{Ghmatecd\_Sigmaec}$ can be parallelized with 
a simple OpenMP Parallel for clause, and it will calculate the remaining $H$ and $G$. 

\section{Methodology}

In order to check if the final result obtained by the parallel program is numerically 
compatible with the original, the concept of matrix norms are necessary. 
Let %$x \in \Cfield^n$ and 
$A \in \Cfield^{m \times n}$. \cite{watkins:2004} defines  
matrix 1-norm as:
\begin{equation}
%	\norm{x}_{\infty} = \max\limits_{1 \leq k \leq n} |x_k| \qquad 
%	\norm{A}_{\infty} = \max\limits_{1 \leq i \leq m} \sum_{j=1}^{n} |a_{ij}| \quad
	\norm{A}_{   1  } = \max\limits_{1 \leq j \leq n} \sum_{i=1}^{m} |a_{ij}| \quad
\end{equation}

All norms have the propierty that $\norm{A} = 0$ if and only if $A = 0$.
Let $f$ and $g$ be two numerical 
algorithms that solves the same problem, but in a different fashion. 
Let now $y_f$ be the result computed by $f$ and $y_g$ be the result computed by
$g$. The \textit{error} between those two values can be measured computing
$\norm{y_f - y_g}$.

The error between CPU and GPU versions of $H$ and $G$ matrices were computated by calulating $\norm{H_{cpu} - H_{gpu}}_1$
and $\norm{G_{cpu} - G_{gpu}}_1$. An automated test check if this value is bellow $10^{-4}$.

For experimenting, there were four data samples as illustrated in Table 1. The 
application is executed in a computer with an AMD A10-7700K processor aided with 
a GeForce 980GTX for each one of the samples using the original code (serial implementation), 
the OpenMP version, and the CUDA and OpenMP together. 
Before any data collection, a warm up procedure is executed, which 
consists of running the application with the sample three times without getting any result. 
Afterward, all experiments were executed 30 times per sample. Each execution produces a 
file with total time elapsed. GPU total time was computed by the sum of 6 elements: 
(1) total time to move data to GPU, (2) launch and execute the kernel, (3) elapsed time 
to compute the result, (4) time to move data back to main memory, (5) time to compute 
the singular part in the CPU. The elapsed time was computed in seconds with the OpenMP library function 
$\texttt{OMP\_GET\_WTIME}$. This function calculates the elapsed wall clock time in seconds 
with double precision. All experiments set the Gauss Quadrature Points to 8.

\begin{table}[]
\centering
\caption{Table 1: Data experiment set}
\label{my-label}
\begin{tabular}{|l|l|l|l|l|}
\hline
Number of Mesh elements    & 240 & 960 & 2160 & 4000 \\ \hline
Number of Bondary elements & 100 & 400 & 900  & 1600 \\ \hline
\end{tabular}
\end{table}

\section{Results}

Gfortran 5.4.0 and CUDA 8.0 were used to compile the application. The main flags used in Gfortran are
-Ofast -funroll-loops -flto . The flags used in
CUDA nvcc compiler are: -use\_fast\_math  -O3  -Xptxas --opt-level=3  -maxrregcount=32 -Xptxas 
--allow-expensive-optimizations=true . 

The logarithmic scale graphic at figure 1 illustrates the results. All points are the mean of the 
time in seconds of 30 executions as described in Methodology.

\begin{figure}[ht]
\centering
\includegraphics[scale=1.0]{results1.pdf}
\caption{Time elapsed by each implementation in logarithm scale}
\label{fig:graphic1}
\end{figure}

Since the speedup acquired in the 4000 mesh elements sample with OpenMP and CUDA+OpenMP with respect to the sequential 
algorithm are $2.26$ and $37.95$ respectively, the conclusion is that the presented strategy aided with GPUs can be 
used to accelerate the overall performance of the simulation for a large number of mesh elements. Notice that 
the performance gain in the 260 sample between OpenMP and CUDA+OpenMP is not noticeable, thus the usage of GPUs 
for smaller data may not be attractive. 


\section{Future Works}
The current implemented code have limitations. First, there is no logic to assemble $H$ and $G$ by blocks by lauching 
multiple kernels. This strategy would allow bigger problems to be solved, since if the number of mesh elements is 
bigger enough the application would crash due to insufficient video memory.  Second, the singular and nonsingular 
part can be computed independently, so the CPU can be used to compute the singular part while the GPU is computing 
the nonsingular part. The usage of GPUs in the singular case can also be analyzed.

%\begin{figure}[ht]
%\centering
%\includegraphics[width=.5\textwidth]{fig1.jpg}
%\caption{A typical figure}
%\label{fig:exampleFig1}
%\end{figure}
%
%\begin{figure}[ht]
%\centering
%\includegraphics[width=.3\textwidth]{fig2.jpg}
%\caption{This figure is an example of a figure caption taking more than one
%  line and justified considering margins mentioned in Section~\ref{sec:figs}.}
%\label{fig:exampleFig2}
%\end{figure}


\bibliographystyle{sbc}
\bibliography{sbc-template}

\end{document}
