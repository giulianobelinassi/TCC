\documentclass[12pt]{article}

\usepackage{sbc-template}

\usepackage{graphicx,url}

%\usepackage[brazil]{babel}   
\usepackage[latin1]{inputenc}  

\usepackage{amsmath}
\usepackage{amssymb} 
\usepackage{mathtools}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

 
\newcommand{\Cfield}{\mathbb{C}}
\newcommand{\Rfield}{\mathbb{R}}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\sloppy

\title{Optimizing a Boundary Elements Method for Stationary
Elastodynamic Problems implementation with GPUs}

\author{Giuliano A. F. Belinassi\inst{1}, Rodrigo Siqueira\inst{1}, Ronaldo Carrion\inst{2}
, Alfredo Goldman\inst{1}, \\ Marco D. Gubitoso\inst{1} }


\address{Instituto de Matemática e Estatística (IME) -- Universidade de São Paulo
  (USP)\\
  Rua do Matão, 1010 -- São Paulo -- SP -- Brazil
\nextinstitute
  Escola Politécnica (EP)  -- Universidade de São Paulo (USP)\\
  Avenida Professor Mello Moraes, 2603 -- São Paulo -- SP -- Brazil
}

\begin{document} 

\maketitle

\begin{abstract}

The Boundary Element Method requires a geometry discretization to execute simulations, 
and it can be used to analyze the 3D stationary behavior of wave propagation in the soil. 
Such discretization involves generating two high computational power demanding matrices, 
and this article demonstrates how Graphical Processing Units (GPU) were used to accelerate 
this process. In a experiment with 4000 Mesh elements and 1600 Boundary elements, 
a speedup of 107$\times$ was obtained with a GeForce GTX980. 

\end{abstract}
     
\section{Introduction}

Differential equations governing problems of Mathematical Physics have analytical 
solutions only in cases in which the domain geometry, boundary and initial conditions are
reasonably simple. Problems with arbitrary domains and fairly general boundary conditions 
can only be solved approximately, for example, by using numerical techniques. 
These techniques were strongly developed due to the presence of increasingly powerful computers, 
enabling the solution of complex mathematical problems.

The Boundary Element Method (BEM) is a very efficient alternative for modeling unlimited domains since
it satisfies the Sommerfeld radiation condition, also known as geometric damping 
\cite{katsikadelis:2016}. This method can be used for numerically modeling the stationary behavior of 3D 
wave propagation in the soil and it is useful as a computational tool to aid in the analysis of soil vibration
\cite{dominguez:1993}. A BEM based tool can be used for analyzing the vibration created 
by heavy machines, railway lines, earthquakes, or even to aid the design of offshore oil platforms.

With the advent of GPUs, several mathematical and engineering simulation problems were redesigned
to be implemented into these massively parallel devices. However, 
first GPUs were designed to render graphics in real time, as a consequence, all the available 
libraries, such as OpenGL, were graphical oriented. These redesigns involved converting 
the original problem to the graphics domain and required expert knowledge of the selected 
graphical library. 

NVIDIA noticed a new demand for their products and created an API called CUDA to enable 
the use of GPUs for general purpose programming. CUDA uses the concept of kernels, which are
functions called from the host to be executed by GPU threads. Kernels are organized into a set of blocks composed of a set 
of threads that cooperate with each other \cite{patterson:2007}.

The memory of a NVIDIA GPU is divided in global memory, local memory, and shared memory.
Global memory is accessible by all threads, local memory is private to a thread and shared 
memory is low-latency and accessible by all threads in a block\cite{patterson:2007}. CUDA 
provides mechanisms to access all of them.

Regarding this work, this parallelization approach is useful because an analysis of a large domain 
requires a proportionally large number of mesh elements, and processing a single element have a high
time cost. Doing such analysis in parallel reduces the computational time requied for the entire
program because multiple elements are processed at the same time. This advantage was provided by this
research.

Before discussing any parallelization technique or results, Section 2 presents a very brief mathematical 
description of BEM for Stationary Elastodynamic Problems and the meaning of some functions presented in 
this paper. Section 3 shows how the most computational intensive routine was optimized using GPUs. 
Section 4 discusses how the results were obtained. Section 5 presents and discusses the results. Finally, 
Section 6 provides an overview of our future work.

\section{Boundary Elements Method Background}

Without addressing details on BEM formulation, the Boundary Integral Equation for Stationary
Elastodynamic Problems can be written as:

$\vspace{-1em}$
\begin{equation}
	c_{ij}u_{j}(\xi,\omega) + \int_S t_{ij}^*(\xi, x, \omega)u_j (x, \omega)\text{d}S(x) = \int_S u_{ij}^*(\xi, x, \omega) t_j(x, \omega)\text{d}S(x) \label{bem_formulation}
\end{equation}

After performing the geometry discretization, Equation ($\ref{bem_formulation}$) can be represented in matrix form as: 
%$\vspace{-0.5em}$
\begin{equation}
	Hu = Gt \label{eqmatrix}
\end{equation}
Functions $u_{ij}^{*}(\xi, x, \omega)$ and $t_{ij}^{*}(\xi, x, \omega)$ (called fundamental solutions)
present a singular behavior when $\xi = x$ ordely $O(1/r)$, called weak singularity, and $O(1/r^2)$,
called strong singularity, respectively. The $r$ value represents the distance between $x$ and $\xi$
points. The integral of these functions, as seen in Eq. ($\ref{bem_formulation}$), will generate the $G$ and $H$ matrices respectively,
as is shown in Eq. ($\ref{eqmatrix}$).
For computing these integrals numerically, the Gaussian quadrature can be deployed. Briefly, it is an algorithm that approximates
integrals by sums as shown in equation ($\ref{quadrature}$)\cite{ascher:2011}, where $g$ is the number of Gauss quadrature points.

$\vspace{-1.2em}$
\begin{equation}
	\int_{a}^{b} f(x)\text{d}x \approx \sum_{i = 1}^{g}w_i f(x_i) \label{quadrature}
\end{equation}

To overcome the mentioned problem in the strong singularity, one can use the artifice known as Regularization of 
the Singular Integral, expressed as follows:
%
\begin{equation}
\begin{split}
	c_{ij}(\xi)u_{j}(\xi, \omega) + \int_{S}\left[t_{ij}^{*}(\xi, x, \omega)_{\text{DYN}} - t_{ij}^{*}(\xi, x)_{\text{STA}} \right]u_{j}(x, \omega) \text{d}S(x) + \\
	+ \int_S t_{ij}(\xi, x)_\text{STA} u_j(x)\text{d}S(x) = \int_S u_{ij}^{*}(\xi, x, \omega)_{\text{DYN}} t_j(x, \omega)\text{d}S(x) \label{singular}
\end{split}
\end{equation}
Where DYN = Dynamic, STA = Static. The integral of the difference between the dynamic and static nuclei, 
the first term in Equation ($\ref{singular}$), does not present singularity when executed concomitantly as expressed because 
they have the same order in the both problems.

Algorithmically, equation ($\ref{bem_formulation}$) is implemented into a routine named $\texttt{Nonsingd}$, computing the
integral using the Gaussian Quadrature without addressing problems related to singularity. 
To overcome singularity problems, there is a special routine called $\texttt{Sing\_de}$ that uses the
artifice described in equation ($\ref{singular}$). Lastly, $\texttt{Ghmatecd}$ is a routine 
developed to create both the $H$ and $G$ matrices described in equation ($\ref{eqmatrix}$). Both $\texttt{Nonsingd}$
and $\texttt{Sing\_de}$ are called from $\texttt{Ghmatecd}$ routine.

\section{Parallelization Strategies}

A parallel implementation of BEM began by analyzing and modifying a sequential code provided by \cite{carrion:02}. 
Gprof, a profiling tool by \cite{binutils}, revealed the two most time-consuming routines: $\texttt{Ghmatecd}$ and 
$\texttt{Nonsingd}$, with $60.9\%$ and $58.3\%$ of the program total elapsed time, respectively. 
Since most calls to $\texttt{Nonsingd}$ were performed inside $\texttt{Ghmatecd}$, most of the parallelization effort was focused 
on that last routine.

%A parallel implementation of BEM began by analyzing and modifying a sequential code 
%provided by \cite{carrion:02}. Gprof \cite{binutils}, a profiling tool by GNU, revealed the two most time-consuming 
%routines as shown in Table \ref{gprof-output}. Since most calls to $\texttt{Nonsingd}$ were from $\texttt{Ghmatecd}$, most of the parallelization effort 
%was focused on that last routine.

%\begin{table}[]
%\centering
%\caption{Gprof Output for 960 Mesh Elements}
%\label{gprof-output}
%\begin{tabular}{|l|l|l|}
%\hline
%Name & Nonsingd & Ghmatecd \\ \hline
%Time & 58.3\%   & 60.9\%   \\ \hline
%\end{tabular}
%\end{table}

\subsection{$\texttt{Ghmatecd}$ Parallelization}

Algorithm 1 shows pseudocode for the $\texttt{Ghmatecd}$ subroutine. Let $n$ be the number of mesh 
elements and $m$ the number of boundary elements. $\texttt{Ghmatecd}$ builds matrices $H$ and $G$ by computing 
smaller $3\times3$ matrices returned by $\texttt{Nonsingd}$ and $\texttt{Sing\_de}$.
%
\begin{algorithm}[H]
\label{ghmatecd_old}
\caption{Creates $H, G \in \Cfield^{(3m)\times(3n)}$}
\begin{algorithmic}[1]
	\Procedure{Ghmatecd}{}
		\For{$j := 1, n$} 
			\For{$i := 1, m$}
				\State{$ii := 3(i-1) + 1;     jj := 3(j-1) + 1$}
				\If{$i == j$}
					\State{$Gelement, Helement \leftarrow \text{Sing\_de}(i)$}\Comment{two $3\times3$ complex matrices}					
				\Else
					\State{$Gelement, Helement \leftarrow \text{Nonsingd}(i, j)$}	
				\EndIf
				\State{$G[ii:ii+2][jj:jj+2] \leftarrow Gelement$}
				\State{$H[ii:ii+2][jj:jj+2] \leftarrow Helement$}
			\EndFor
	 \EndFor
	\EndProcedure
\end{algorithmic}
\end{algorithm}
%
There is no interdependency between all iterations of the loops in lines 2 and 3, so all iterations can be computed 
in parallel. Since typically a modern high-end CPU have 8 cores, even a small number of mesh elements generate enough 
workload to use all CPUs resources if this strategy alone is used. On the other hand, a typical GPU contain thousands 
of processors, hence even a considerable large amount of elements may not generate a workload that consumes all the device's 
resources. Since $\texttt{Nonsingd}$ is the cause of the performance bottleneck of $\texttt{Ghmatecd}$, 
the main effort was implementing
an optimized version of $\texttt{Ghmatecd}$, called $\texttt{Ghmatecd\_Nonsingd}$, that only computes the $\texttt{Nonsingd}$
case in the GPU, and leave $\texttt{Sing\_de}$ to be computed in the CPU after the computation of $\texttt{Ghmatecd\_Nonsingd}$ 
is completed. The pseudocode in Algorithm \ref{ghmatecd_new} pictures a new strategy where $\texttt{Nonsingd}$ is also computed in parallel.
Let $g$ be the number of Gauss quadrature points.

\begin{algorithm}[H]
\caption{Creates $H, G \in \Cfield^{(3m)\times(3n)}$ }
\label{ghmatecd_new}
\begin{algorithmic}[1]
	\Procedure{Ghmatecd\_nonsingd}{}
		\For{$j := 1, n$} 
			\For{$i := 1, m$}
				\State{$ii := 3(i-1) + 1;     jj := 3(j-1) + 1$}
				\State{Allocate \textit{Hbuffer} and \textit{Gbuffer}, buffer of matrices $3 \times 3$ of size $g^2$}
				\If{$i \neq j$}
					\For{$y := 1, g$}
						\For{$x := 1, g$}
							\State{$\textit{Hbuffer}(x, y) \leftarrow \text{GenerateMatrixH}(i, j, x, y)$}
							\State{$\textit{Gbuffer}(x, y) \leftarrow \text{GenerateMatrixG}(i, j, x, y)$}
						\EndFor
					\EndFor
				\EndIf
				\State{$Gelement \leftarrow \text{SumAllMatricesInBuffer}(\textit{Gbuffer})$} 
				\State{$Helement \leftarrow \text{SumAllMatricesInBuffer}(\textit{Hbuffer})$}
				\State{$G[ii:ii+2][jj:jj+2] \leftarrow Gelement$}
				\State{$H[ii:ii+2][jj:jj+2] \leftarrow Helement$}
			\EndFor
	 \EndFor
	\EndProcedure
	\Procedure{Ghmatecd\_Sing\_de}{}
		\For{$i := 1, m$}
			\State{$ii := 3(i-1) + 1$}
			\State{$Gelement, Helement \leftarrow \text{Sing\_de}(i)$}	
			\State{$G[ii:ii+2][ii:ii+2] \leftarrow Gelement$}
			\State{$H[ii:ii+2][ii:ii+2] \leftarrow Helement$}
	 \EndFor
	\EndProcedure
	\Procedure{Ghmatecd}{} 
		\State{$\text{Ghmatecd\_Nonsingd}()$}
		\State{$\text{Ghmatecd\_Sing\_de}()$}
	\EndProcedure
		
\end{algorithmic}
\end{algorithm}

%\begin{algorithm}
%\caption{Creates $H, G \in \Cfield^{(3m)\times(3n)}$ without Sing_de part}
%\begin{algorithmic}[1]
%	\Procedure{Ghmatecd\_nonsingd}{}
%		\For{$j := 1, n$} 
%			\For{$i := 1, m$}
%				\State{$ii := 3(i-1) + 1;     jj := 3(j-1) + 1$}
%				\State{Allocate \textit{Hbuffer} and \textit{Gbuffer}, buffer of matrices $3 \times 3$ of size $g^2$}
%				\If{$i \neq j$}
%					\For{$y := 1, g$}
%						\For{$x := 1, g$}
%							\State{$\textit{Hbuffer}(x, y) \leftarrow \text{GenerateMatrixH}(i, j, x, y)$}
%							\State{$\textit{Gbuffer}(x, y) \leftarrow \text{GenerateMatrixG}(i, j, x, y)$}
%						\EndFor
%					\EndFor
%					\State{$Gelement \leftarrow \text{SumAllMatricesInBuffer}(\textit{Gbuffer})$} 
%					\State{$Helement \leftarrow \text{SumAllMatricesInBuffer}(\textit{Hbuffer})$}
%					\State{$G[ii:ii+2][jj:jj+2] \leftarrow Gelement$}
%					\State{$H[ii:ii+2][jj:jj+2] \leftarrow Helement$}
%				\EndIf
%			\EndFor
%	 \EndFor
%	\EndProcedure
%\end{algorithmic}
%\end{algorithm}
%
%\begin{algorithm}
%\caption{Compute Sing_de part of $H, G \in \Cfield^{(3m)\times(3n)}$}
%\begin{algorithmic}[1]
%	\Procedure{Ghmatecd\_Sing_de}{}
%		\For{$i := 1, m$}
%			\State{$ii := 3(i-1) + 1$}
%			\State{$Gelement, Helement \leftarrow \text{Sing_de}(i)$}	
%			\State{$G[ii:ii+2][ii:ii+2] \leftarrow Gelement$}
%			\State{$H[ii:ii+2][ii:ii+2] \leftarrow Helement$}
%	 \EndFor
%	\EndProcedure
%\end{algorithmic}
%\end{algorithm}

The $\texttt{Ghmatecd\_Nonsingd}$ routine can be implemented as a CUDA kernel. In a CUDA block, $g \times g$ 
threads are created to compute in parallel the two nested loops in lines 2 and 3, allocating spaces in the shared 
memory to keep the matrix buffers $\texttt{Hbuffer}$ and $\texttt{Gbuffer}$. Since these buffers contain matrices of size 
$3 \times 3$, nine of these $g \times g$ threads can be used to 
sum all matrices, because one thread can be assigned to each matrix entry, unless $g < 3$. Note that $g$ is also upper-bounded by the 
amount of shared memory available in the GPU. Launching $m \times n$ blocks to cover the two nested loops in lines
2 to 3 will generate the entire $H$ and $G$ without the $\texttt{Sing\_de}$ part. The $\texttt{Ghmatecd\_Sing\_de}$ routine can be parallelized with 
a simple OpenMP $\texttt{Parallel for}$ clause, and it will compute the remaining $H$ and $G$. 

\section{Methods}

Matrix norms were used to assert the correctness of our results. 
Let %$x \in \Cfield^n$ and 
$A \in \Cfield^{m \times n}$. \cite{watkins:2004} defines  
matrix 1-norm as:
\begin{equation}
%	\norm{x}_{\infty} = \max\limits_{1 \leq k \leq n} |x_k| \qquad 
%	\norm{A}_{\infty} = \max\limits_{1 \leq i \leq m} \sum_{j=1}^{n} |a_{ij}| \quad
	\norm{A}_{   1  } = \max\limits_{1 \leq j \leq n} \sum_{i=1}^{m} |a_{ij}| \quad
\end{equation}

All norms have the property that $\norm{A} = 0$ if and only if $A = 0$.
Let $u$ and $v$ be two numerical 
algorithms that solve the same problem, but in a different way. 
Now let $y_u$ be the result computed by $u$ and $y_v$ be the result computed by
$v$. The \textit{error} between these two values can be measured computing
$\norm{y_u - y_v}$. The error between CPU and GPU versions of $H$ and $G$ matrices was computed by calculating $\norm{H_{cpu} - H_{gpu}}_1$
and $\norm{G_{cpu} - G_{gpu}}_1$. An automated test check if this value is bellow $10^{-4}$.

Gfortran 5.4.0 and CUDA 8.0 were used to compile the application. The main flags used in Gfortran were
$\texttt{-Ofast}$ $\texttt{-march=native}$ $\texttt{-funroll-loops}$ $\texttt{-flto}$. The flags used in
CUDA nvcc compiler were: $\texttt{-use\_fast\_math}$  $\texttt{-O3}$ $\texttt{-Xptxas}$ $\texttt{--opt-level=3}$
$\texttt{-maxrregcount=32}$ $\texttt{-Xptxas}$ 
$\texttt{--allow-expensive-optimizations=true}$ . 

For experimenting, there were four data samples as shown in Table \ref{experiments}. The application was 
executed for each sample using the original code (serial implementation), the OpenMP 
version and the CUDA and OpenMP together. All tests but the sequential set the number of OpenMP 
threads to 4. The machine used in all experiments had an AMD A10-7700K processor paired with a 
GeForce GTX980\footnote{Thanks to NVIDIA for donating this GPU.}.

Before any data collection, a warm up procedure is executed, which consists of running the 
application with the sample three times without getting any result. Afterward, all experiments 
were executed 30 times per sample. Each execution produced a file with total time elapsed, 
where a script computed averages and standard deviations for all experiments.

GPU total time was computed by the sum of 5 elements: 
(1) total time to move data to GPU, (2) launch and execute the kernel, (3) elapsed time 
to compute the result, (4) time to move data back to main memory, (5) time to compute 
the remaining $H$ and $G$ parts in the CPU. 
The elapsed time was computed in seconds with the OpenMP library function 
$\texttt{OMP\_GET\_WTIME}$. This function calculates the elapsed wall clock time in seconds 
with double precision. All experiments set the Gauss Quadrature Points to 8.

\begin{table}[]
\centering
\caption{Data experiment set}
\label{experiments}
\begin{tabular}{|l|l|l|l|l|}
\hline
Number of Mesh elements     & 240 & 960 & 2160 & 4000 \\ \hline
Number of Boundary elements & 100 & 400 & 900  & 1600 \\ \hline
\end{tabular}
\end{table}

\section{Results}


The logarithmic scale graphic at Figure 1 illustrates the results. All points are the mean of the 
time in seconds of 30 executions as described in Methodology. The average is meaningful as the maximum standard deviation
obtained was $2.6\%$ of the mean value.

\begin{figure}[ht]
\centering
\includegraphics[scale=0.866]{results1.pdf}
\caption{Time elapsed by each implementation in logarithm scale}
\label{fig:graphic1}
\end{figure}

The speedup acquired in the $4000$ mesh elements sample with OpenMP and CUDA+OpenMP with respect to the sequential 
algorithm are $2.7$ and $107$ respectively. As a conclusion, the presented strategy paired with GPUs can be 
used to accelerate the overall performance of the simulation for a large number of mesh elements. This is a consequence 
of parallelizing the construction of both matrices $H$ and $G$, and the calculations in the $\texttt{Nonsingd}$ routine. 
Notice that there was a performance loss in the 260 sample between OpenMP and CUDA+OpenMP, this was caused by the high 
latency between CPU-GPU communication, thus the usage of GPUs may not be attractive for small meshes.


\section{Future Work}
There are issues related to the $g$ described in Algorithm \ref{ghmatecd_new}. Detailed studies are required to determine the 
exact value of $g$ that provides a good relation between precision and performance. Also, better ways to compute the
sum in lines 11-12 of Algorithm 2 may increase performance.  The usage of GPUs for the singular case can also be analyzed.

%The current implemented code have limitations. First, there is no logic to construct both $H$ and $G$ by blocks to create several 
%GPU kernels. Second, there is also no logic to compute both $\texttt{Ghmatecd\_Nonsingd}$ and $\texttt{Ghmatecd\_Sing\_de}$ in 
%parallel with respect to each other. The usage of GPUs in the singular case can also be analyzed.


%\begin{figure}[ht]
%\centering
%\includegraphics[width=.5\textwidth]{fig1.jpg}
%\caption{A typical figure}
%\label{fig:exampleFig1}
%\end{figure}
%
%\begin{figure}[ht]
%\centering
%\includegraphics[width=.3\textwidth]{fig2.jpg}
%\caption{This figure is an example of a figure caption taking more than one
%  line and justified considering margins mentioned in Section~\ref{sec:figs}.}
%\label{fig:exampleFig2}
%\end{figure}


\bibliographystyle{sbc}
\bibliography{sbc-template}

\end{document}
